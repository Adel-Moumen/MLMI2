{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7922c105",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eab98",
   "metadata": {},
   "source": [
    "## Use `dataloader` to get an utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d135fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff22c4",
   "metadata": {},
   "source": [
    "# CTC model for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178724d9",
   "metadata": {},
   "source": [
    "## Obtain phoneme output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10b7c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab_39.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m phonemes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab_39.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m     10\u001b[0m         vocab[text\u001b[38;5;241m.\u001b[39mstrip()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\n",
      "File \u001b[0;32m/rds/project/rds-xyBFuSj0hm0/MLMI2.M2025/miniconda3/envs/mlmi2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab_39.txt'"
     ]
    }
   ],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab_39.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b3baa",
   "metadata": {},
   "source": [
    "## Model & training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa93a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'dev_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffcb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53390d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c3de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from trainer import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {:.2f} minutes.'.format(duration / 60))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea487bd",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056a2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "# model_path=\"checkpoints/20221110_120418/model_16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087018a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
